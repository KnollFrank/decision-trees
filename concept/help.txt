Entscheidungsbäume
==================

- https://machinelearningmastery.com/implement-decision-tree-algorithm-scratch-python/
- https://medium.com/@penggongting/implementing-decision-tree-from-scratch-in-python-c732e7c69aea
- http://sigmajs.org/
- https://lamastex.github.io/scalable-data-science/sds/2/2/db/016_SupervisedClustering_DecisionTrees_HandWrittenDigitRecognition/
- https://transcranial.github.io/keras-js/#/mnist-cnn

C:\Users\Alfa\AppData\Local\Programs\Python\Python38-32\python.exe

Abhängigkeiten:
- https://www.papaparse.com/
- https://visjs.org/ oder https://almende.github.io/vis/
- https://datatables.net/

TODO:
- Progressbar für den aktuellen Stand des Trainings des Entscheidungsbaums
- Laden/Speichern eines Modells für Entscheidungsbäume (= Graph)
- folgendes Umsetzen:
  Thanks for the post, code, and all the follow ups! I also was wondering about avoiding some of the unnecessary splits at the bottom of the tree. In case others are curious, one short addition to the code would be to add
  if len(class_values) == 1: return to_terminal(dataset)
  as the second line of get_split, and then
  if not isinstance(node, dict): return
  at the beginning of split.
  It looks like this avoids unnecessary splitting in the example given and in some other datasets.
  Thanks again!
- I did some modification to deal with categorical variables,
in get_split()

	if isinstance(row[index], Number):
		groups = test_split_numeric(index, row[index], dataset)
	else:
		groups = test_split_categorical(index, row[index], dataset)

where
def test_split_categorical(index, value, dataset):
	left, right = list(), list()
	for row in dataset:
		if row[index] == value:
			left.append(row)
		else:
			right.append(row)
	return left, right


, in print_tree()

	if isinstance(node['value'], Number):
		print('%s[X%d < %.3f]' % ((depth*' ', (node['index']+1), node['value'])))
	else:
		print('%s[X%d = %s]' % ((depth*' ', (node['index']+1), node['value'])))

, and in predict()

	if isinstance(node['value'], Number):
		split_condition = (row[node['index']] < node['value'])
	else:
		split_condition = (row[node['index']] == node['value'])

I ran the program on another dataset (Loan Prediction) and it worked fine with about 79% accuracy.
- Bei "Dateneingabe -> vorhersagen" sollte der Pfad, der zu einer Vorhersage aufgrund
  der eingegebenen Werte führt, im Entscheidungsbaum farblich hervorgehoben werden.
- Eingabe eines neuen Datensatzes
- Aufteilung des Datensatzes in 80% Trainingsdaten und 20% Testdaten
